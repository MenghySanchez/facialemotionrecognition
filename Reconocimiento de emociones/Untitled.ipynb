{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03de2599-e6bb-4dca-9841-ca8826e75b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "from PIL import Image\n",
    "from PIL import ImageTk\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib\n",
    "import os\n",
    "\n",
    "# Variables para calcular FPS\n",
    "time_actualframe = 0\n",
    "time_prevframe = 0\n",
    "\n",
    "# Tipos de emociones del detector\n",
    "classes = ['enfadado','disgustado','temor','feliz','neutral','triste','sorprendido']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4fb9f32-f425-477e-98c8-307a42651f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el reconocedor de caras preentrenado.\n",
    "prototxtPath = r\"face_detector\\deploy.prototxt\"\n",
    "weightsPath = r\"face_detector\\res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
    "\n",
    "# Carga el detector de clasificación de emociones\n",
    "emotionModel = load_model(\"modelFEC.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "286732f5-0388-4199-81b3-23d0a33e5ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando una función para elegir el video de entrada o video en directo mediante la selección de los radiobuttons\n",
    "def video_de_entrada():\n",
    "    global cam\n",
    "    if selected.get() == 1:\n",
    "        path_video = filedialog.askopenfilename(filetypes = [\n",
    "            (\"all video format\", \".mp4\"),\n",
    "            (\"all video format\", \".avi\")])\n",
    "        if len(path_video) > 0:\n",
    "            btnEnd.configure(state=\"active\")\n",
    "            rad1.configure(state=\"disabled\")\n",
    "            rad2.configure(state=\"disabled\")\n",
    "\n",
    "            pathInputVideo = \"...\" + path_video[-20:]\n",
    "            lblInfoVideoPath.configure(text=pathInputVideo)\n",
    "            cam = cv2.VideoCapture(path_video)\n",
    "            visualizar()\n",
    "    if selected.get() == 2:\n",
    "        btnEnd.configure(state=\"active\")\n",
    "        rad1.configure(state=\"disabled\")\n",
    "        rad2.configure(state=\"disabled\")\n",
    "        lblInfoVideoPath.configure(text=\"\")\n",
    "        cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "        visualizar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa45117-530e-4135-bd09-a307e8f8c96d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a83faa-54ba-4415-8359-698bc45a588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toma la imagen, los modelos de detección de rostros y mascarillas \n",
    "# Retorna las localizaciones de los rostros y las predicciones de emociones de cada rostro\n",
    "def predict_emotion(frame,faceNet,emotionModel):\n",
    "# Construye un blob de la imagen\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (224, 224),(104.0, 177.0, 123.0))\n",
    "\n",
    "    # Realiza las detecciones de rostros a partir de la imagen\n",
    "    faceNet.setInput(blob)\n",
    "    detections = faceNet.forward()\n",
    "\n",
    "    # Listas para guardar rostros, ubicaciones y predicciones\n",
    "    faces = []\n",
    "    locs = []\n",
    "    preds = []\n",
    "\n",
    "    # Recorre cada una de las detecciones\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        \n",
    "        # Fija un umbral para determinar que la detección es confiable\n",
    "        # Tomando la probabilidad asociada en la deteccion\n",
    "\n",
    "        if detections[0, 0, i, 2] > 0.4:\n",
    "            # Toma el bounding box de la detección escalado\n",
    "            # de acuerdo a las dimensiones de la imagen\n",
    "            box = detections[0, 0, i, 3:7] * np.array([frame.shape[1], frame.shape[0], frame.shape[1], frame.shape[0]])\n",
    "            (Xi, Yi, Xf, Yf) = box.astype(\"int\")\n",
    "\n",
    "            # Valida las dimensiones del bounding box\n",
    "            if Xi < 0: Xi = 0\n",
    "            if Yi < 0: Yi = 0\n",
    "            # Se extrae el rostro y se convierte BGR a GRAY\n",
    "            # Finalmente se escala a 224x244\n",
    "            face = frame[Yi:Yf, Xi:Xf]\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            face = cv2.resize(face, (48, 48))\n",
    "            face2 = img_to_array(face)\n",
    "            face2 = np.expand_dims(face2,axis=0)\n",
    "\n",
    "            # Se agrega los rostros y las localizaciones a las listas\n",
    "            faces.append(face2)\n",
    "            locs.append((Xi, Yi, Xf, Yf))\n",
    "\n",
    "            pred = emotionModel.predict(face2)\n",
    "            preds.append(pred[0])\n",
    "\n",
    "    return (locs,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49036b22-2dcd-40d9-9784-811b2b7a70f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
